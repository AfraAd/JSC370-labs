---
title: "Lab 08 - Text Mining/NLP"
output: html_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/ available at https://github.com/JSC370/JSC370-2025/tree/main/data/medical_transcriptions.

# Deliverables

1. Questions 1-7 answered, knit to pdf or html output uploaded to Quercus.

2. Render the Rmarkdown document using `github_document` and add it to your github site. Add link to github site in your html.


### Setup packages

You should load in `tidyverse`, (or `data.table`), `tidytext`, `wordcloud2`, `tm`, and `topicmodels`.


## Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r eval=FALSE}
library(tidytext)
library(tidyverse)
library(wordcloud2)
library(tm)
library(topicmodels)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples |>
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r eval=FALSE}
mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(x = fct_reorder(medical_specialty, n), y=n)) +
  geom_col(fill="lightblue") +
  coord_flip()
```

**Answer:** The number of medical specialists exponentially increased from the lowest number of specialties to the highest with surgery. The categories are unrelated and generally do not overlap.

---

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud of the top 20 most frequent words

### Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r eval=FALSE}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  group_by(word) |>
  summarise(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

tokens |>
  ggplot(aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat = "identity",fill="lightblue")+
  coord_flip()

tokens |>
  count(word, sort=TRUE) |>
  wordcloud2(size=0.4, color="random-light")
```

**Answer:** We see that stop words such as "the", "and", and "was" are the most common terms, and the word frequency exponentially increases as we move up the word's order. This result makes sense as stop words are the most commonly used words in a sentence regardless of the topic, so these words are not very useful in explaining the content. We should remove them.

---

## Question 3: Stopwords

- Redo Question 2 but remove stopwords
- Check `stopwords()` library and `stop_words` in `tidytext`
- Use regex to remove numbers as well
- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

### What do we see when you remove stopwords and then when you filter further? Does it give us a better idea of what the text is about?

```{r eval=FALSE}
head(stopwords("english"))
length(stopwords("english"))
head(stop_words)

tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  anti_join(stop_words, by = "word") |>
  filter(!str_detect(word, "^[0-9]+$") & !word %in% c("mm", "mg", "", "noted")) |>
  group_by(word) |>
  summarise(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

ggplot(data=tokens, aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  coord_flip()

tokens |>
  count(word, sort=TRUE) |>
  wordcloud2(size=0.4, color="random-light")
```

**Answer:** After filtering for stopwords, numbers, and other meaningless terms, we get mainly medical words, which give us a better idea of the most common medical terms in our dataset. We can use this to better determine the specific medical topics in our dataset.

---


## Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r eval=FALSE}
stopwords2 <- c(stop_words$word, "mm", "mg", "noted")

sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")

tokens_bigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
  filter(!grepl(sw_start, ngram, ignore.case = TRUE))|>
  filter(!grepl(sw_end, ngram, ignore.case = TRUE))|>
  filter(!grepl("[[:digit:]]+", ngram))|>
  group_by(ngram) |>
  summarise(word_frequency=n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

tokens_bigram |>
  ggplot(aes(ngram, word_frequency)) +
  geom_col(fill="lightblue") +
  coord_flip()
```

**Answer:** Though we don't perform tri-grams, we can see the word frequency decrease for the bi-gram pairs, so the word frequency will be even less for tri-gram pairs.

---

## Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.

```{r eval=FALSE}
library(stringr)
# e.g. patient, blood, preoperative...
tokens_bigram |>
  filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |>
    mutate(word = str_remove(ngram, "blood"),
         word = str_remove_all(word, " ")) |>
  group_by(word) |>
  head(20) |>
  ggplot(aes(reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat = "identity", fill="lightblue") +
  coord_flip()
```

---


## Question 6: Words by Specialties

Which words are most used in each of the specialties? You can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the 5 most used words?


```{r eval=FALSE}
mt_samples |>
  unnest_tokens(word, transcription) |>
  anti_join(stop_words, by="word") |>
  filter(!str_detect(word, "[[:digit:]]+")) |>
  filter(!word %in% c("mm", "mg", "noted")) |>
  group_by(medical_specialty) |>
  count(word, sort=TRUE) |>
  head(20)
```

**Answer:** The top words, such as patients, appear in different medical specialties, and the top 5 words are in surgery.


## Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)

```{r eval=FALSE}
# With k=5
transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  anti_join(stop_words, by="word") |>
  filter(!str_detect(word, "[[:digit:]]+")) |>
  filter(!word %in% c("mm", "mg", "noted")) |>
  DocumentTermMatrix()


transcripts_dtm <- as.matrix(transcripts_dtm)   

transcripts_lda <- LDA(transcripts_dtm, k=5, control = list(seed=4321))

```

```{r eval=FALSE}
library(reshape2)

transcripts_top_terms <-
  tidy(transcripts_lda, matrix="beta") |>
  filter(!str_detect(term, "[[:digit:]]+")) |>
  group_by(topic) |>
  slice_max(beta, n=10) |>
  ungroup() |>
  arrange(topic, beta)

transcripts_top_terms |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill=factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~topic, scales="free") +
  scale_y_reordered() +
  theme_bw()
```

```{r eval=FALSE}
# With k = 2
transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  anti_join(stop_words, by="word") |>
  filter(!str_detect(word, "[[:digit:]]+")) |>
  filter(!word %in% c("mm", "mg", "noted")) |>
  DocumentTermMatrix()


transcripts_dtm <- as.matrix(transcripts_dtm)   

transcripts_lda <- LDA(transcripts_dtm, k=2, control = list(seed=4321))

```

```{r eval=FALSE}
transcripts_top_terms <-
  tidy(transcripts_lda, matrix="beta") |>
  filter(!str_detect(term, "[[:digit:]]+")) |>
  group_by(topic) |>
  slice_max(beta, n=10) |>
  ungroup() |>
  arrange(topic, beta)

transcripts_top_terms |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill=factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~topic, scales="free") +
  scale_y_reordered() +
  theme_bw()
```


